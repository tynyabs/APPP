pip install pycountry

pip install langdetect

from textblob import TextBlob
import sys
import tweepy
import re, itertools
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import os
import nltk
nltk.download('punkt')
nltk.download('vader_lexicon')
import pycountry
import re
import string
from wordcloud import WordCloud, STOPWORDS
from PIL import Image
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from langdetect import detect
from nltk.stem import SnowballStemmer
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from sklearn.feature_extraction.text import CountVectorizer

# Authentication
consumerKey = 'IWiL9hPr7cUWaArY9L3CrKvI5'
consumerSecret = 'Vr0BRvRMiRhedsvE9jqSd4CZsEyJe63evVLwma8Ok7Ll5F6nYW'
accessToken = '1404774633778429961-JAbes6LnDJkJQVqWBhAT1ILRKh6Amf'
accessTokenSecret = '16N5bFJ4OLkeUL7dnYD8Q6xWmpE0twoJPu8Xr5fl57S3e'
auth = tweepy.OAuthHandler(consumerKey, consumerSecret)
auth.set_access_token(accessToken, accessTokenSecret)
api = tweepy.API(auth)

def percentage(part,whole):
 return 100 * float(part)/float(whole)
 

keyword = input('Please enter keyword or hashtag to search: ')
noOfTweet = int(input ('Please enter how many tweets to analyze: '))

tweets = tweepy.Cursor(api.search,lang ='en', q=keyword).items(noOfTweet)
positive = 0
negative = 0
neutral = 0
polarity = 0
tweet_list = []
neutral_list = []
negative_list = []
positive_list = []


for tweet in tweets:
  final_tweet = tweet.text.replace('RT','')
  
  if final_tweet.startswith(' @'):
    position = final_tweet.index(':')
    final_tweet = final_tweet[position+2:]
  if final_tweet.startswith('@'):
    position = final_tweet.index('')
    final_tweet = final_tweet[position+2:]
  analysis = TextBlob(final_tweet)
  tweet_polarity = analysis.polarity
  if tweet_polarity>0.00:
    positive +=1
  elif tweet_polarity<0.00:
    negative += 1
  elif tweet_polarity==0.00:
    neutral =+ 1
  polarity += tweet_polarity 
  
  print(final_tweet)
  tweet_list.append(final_tweet)

positive = percentage(positive, noOfTweet)
negative = percentage(negative, noOfTweet)
neutral = percentage(neutral, noOfTweet)
polarity = percentage(polarity, noOfTweet)
positive = format(positive, ".1f")
negative = format(negative, ".1f")
neutral = format(neutral, ".1f")
 

tweet_list

df = pd.DataFrame(tweet_list, columns= ['Tweets'])
df

def clean(text):
  
  text = re.sub(r'@[A-Za-z0-9]','', text)
  text = re.sub(r'#','', text)
  text = re.sub(r'https?:\/\/\S+','', text)
  text = re.sub(r'^ws','',text)
  text = ''.join(''.join(s)[:2] for _, s in
  itertools.groupby(text))
  text = text.lower()
  tokens = nltk.word_tokenize(text)
  return text

df['Tweets']  =  df['Tweets'].apply(clean)

#show
df

#Creating new dataframe and new features
tw_list = pd.DataFrame(tweet_list)
tw_list['text'] = tw_list[0]
tw_list

#Calculating Negative, Positive, Neutral and Compound values
tw_list[["polarity", "subjectivity"]] = tw_list["text"].apply(lambda Text: pd.Series(TextBlob(Text).sentiment))
for index, row in tw_list["text"].iteritems():
 score = SentimentIntensityAnalyzer().polarity_scores(row)
 neg = score["neg"]
 neu = score["neu"]
 pos = score["pos"]
 comp = score["compound"]
 if neg > pos:
  tw_list.loc[index, "sentiment"] = 'negative'
 elif pos > neg:
  tw_list.loc[index, "sentiment"] = 'positive'
 else:
  tw_list.loc[index, "sentiment"] = 'neutral'
 tw_list.loc[index, "neg"] = neg
 tw_list.loc[index, "neu"] = neu
 tw_list.loc[index, "pos"] = pos
 tw_list.loc[index, "compound"] = comp
tw_list

X=tw_list['text']
y=tw_list['sentiment']

from nltk.corpus import stopwords
nltk.download('stopwords')
import string
from nltk.stem import PorterStemmer

stop_words=stopwords.words('english')
punct=string.punctuation
stemmer=PorterStemmer()

import re
cleaned_data=[]
for i in range(len(X)):
  tweet=re.sub('[^a-zA-Z]',' ',X.iloc[i])
  tweet=tweet.lower().split()
  tweet=[stemmer.stem(word) for word in tweet if (word not in stop_words) and (word not in punct)]
  tweet=' '.join(tweet)
  cleaned_data.append(tweet)

cleaned_data

y


sentiment_ordering = ['negative', 'neutral', 'positive']

y = y.apply(lambda x: sentiment_ordering.index(x))

y.head()

from sklearn.feature_extraction.text import CountVectorizer
cv=CountVectorizer(max_features=3000,stop_words=['virginamerica','unit'])
X_fin=cv.fit_transform(cleaned_data).toarray()
X_fin.shape

from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import train_test_split
model=MultinomialNB()

X_train,X_test,y_train,y_test=train_test_split(X_fin,y,test_size=0.3)

model.fit(X_train,y_train)

y_pred=model.predict(X_test)

from sklearn.metrics import classification_report
cf=classification_report(y_test,y_pred)
print(cf)

import pickle
pickle.dump(model,open('model.pkl', 'wb'))

